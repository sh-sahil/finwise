{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DMqMEm6_S9BE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## making the api"
      ],
      "metadata": {
        "id": "Ul-jYECQdvME"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "\n",
        "# List of 10 stock tickers (companies)\n",
        "tickers = ['MSFT', 'AAPL', 'GOOG', 'AMZN', 'TSLA', 'META', 'NFLX', 'NVDA', 'INTC', 'BABA']\n",
        "\n",
        "# Empty list to store all the stock data\n",
        "all_stock_data = []\n",
        "\n",
        "# Download stock data for each company and append to the list\n",
        "for ticker in tickers:\n",
        "    stock_data = yf.download(ticker, start=\"2010-01-01\", end=\"2023-01-01\")\n",
        "\n",
        "    # Add a new column to the data with the ticker name\n",
        "    stock_data['Ticker'] = ticker\n",
        "\n",
        "    # Append the data to the list\n",
        "    all_stock_data.append(stock_data)\n",
        "\n",
        "# Concatenate all the data into one DataFrame\n",
        "combined_stock_data = pd.concat(all_stock_data)\n",
        "\n",
        "# Save the combined data to a CSV file\n",
        "combined_stock_data.to_csv('combined_stock_data.csv')\n",
        "\n",
        "print(\"Stock data saved to 'combined_stock_data.csv'\")\n"
      ],
      "metadata": {
        "id": "DKqXkoDSduxE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "////"
      ],
      "metadata": {
        "id": "WK9NBcUjlkXx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load your data\n",
        "df = pd.read_csv('dataset.csv', parse_dates=['Date'])\n",
        "\n",
        "# Sort by date (in case it's not sorted)\n",
        "df = df.sort_values(by=['Ticker', 'Date'])\n",
        "\n",
        "# Feature Engineering - Adding moving averages as an example\n",
        "df['MA_5'] = df['Close'].rolling(window=5).mean()\n",
        "df['MA_20'] = df['Close'].rolling(window=20).mean()\n",
        "\n",
        "# Drop rows with NaN values created by rolling\n",
        "df = df.dropna()\n",
        "\n",
        "# Use 'Close' as target variable and other features\n",
        "features = ['Open', 'High', 'Low', 'Volume']\n",
        "target = 'Close'\n",
        "\n",
        "# Normalize data\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled_data = scaler.fit_transform(df[features])\n",
        "\n",
        "# Prepare the dataset for time-series forecasting\n",
        "def create_dataset(data, target_col, look_back=60):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - look_back):\n",
        "        X.append(data[i:i+look_back])\n",
        "        y.append(data[i+look_back, target_col])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Create the time-series dataset\n",
        "X, y = create_dataset(scaled_data, df.columns.get_loc(target))\n",
        "\n",
        "# Train-Test Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Build the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "model.add(LSTM(units=50))\n",
        "model.add(Dense(1))\n",
        "\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
        "\n",
        "# Predict and evaluate\n",
        "y_pred = model.predict(X_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLJSpfXTSXuD",
        "outputId": "f7293deb-0f3d-4b45-be05-17966817f5d0"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0125\n",
            "Epoch 2/10\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 7ms/step - loss: 0.0077\n",
            "Epoch 3/10\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - loss: 0.0065\n",
            "Epoch 4/10\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - loss: 0.0060\n",
            "Epoch 5/10\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - loss: 0.0055\n",
            "Epoch 6/10\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - loss: 0.0051\n",
            "Epoch 7/10\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - loss: 0.0049\n",
            "Epoch 8/10\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 0.0047\n",
            "Epoch 9/10\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - loss: 0.0045\n",
            "Epoch 10/10\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - loss: 0.0045\n",
            "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 4ms/step\n",
            "Mean Absolute Error: 138197576.0064876\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Correctly invert scaling for the predictions and true values\n",
        "y_test_rescaled = scaler.inverse_transform(np.concatenate((X_test[:, -1, :-1], y_test.reshape(-1, 1)), axis=1))[:, -1]\n",
        "y_pred_rescaled = scaler.inverse_transform(np.concatenate((X_test[:, -1, :-1], y_pred), axis=1))[:, -1]\n",
        "\n",
        "mae = np.mean(np.abs(y_test_rescaled - y_pred_rescaled))\n",
        "print(f'Mean Absolute Error: {mae}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Pq90cfAaZaa",
        "outputId": "e6f572f4-75a7-46fc-d1a3-b3395effae08"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Absolute Error: 138197576.0064876\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "using LSTM"
      ],
      "metadata": {
        "id": "cRYiIZ9ufFSq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load the data\n",
        "df = pd.read_csv('dataset.csv', parse_dates=['Date'])\n",
        "\n",
        "# Sort by date (in case it's not sorted)\n",
        "df = df.sort_values(by=['Ticker', 'Date'])\n",
        "\n",
        "# Feature Engineering - Adding moving averages as an example\n",
        "df['MA_5'] = df['Close'].rolling(window=5).mean()\n",
        "df['MA_20'] = df['Close'].rolling(window=20).mean()\n",
        "\n",
        "# Drop rows with NaN values created by rolling\n",
        "df = df.dropna()\n",
        "\n",
        "# Use 'Close' as the target variable and other features\n",
        "features = ['Open', 'High', 'Low', 'Volume', 'MA_5', 'MA_20']\n",
        "target = 'Close'\n",
        "\n",
        "# Normalize the data (scaling the features)\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "scaled_data = scaler.fit_transform(df[features])\n",
        "\n",
        "# Prepare the dataset for time-series forecasting\n",
        "def create_dataset(data, target_col, look_back=60):\n",
        "    X, y = [], []\n",
        "    for i in range(len(data) - look_back):\n",
        "        X.append(data[i:i + look_back])\n",
        "        y.append(data[i + look_back, target_col])\n",
        "    return np.array(X), np.array(y)\n",
        "\n",
        "# Create the time-series dataset (X and y)\n",
        "X, y = create_dataset(scaled_data, df.columns.get_loc(target))\n",
        "\n",
        "# Train-Test Split (chronologically)\n",
        "train_size = int(len(X) * 0.8)\n",
        "X_train, X_test = X[:train_size], X[train_size:]\n",
        "y_train, y_test = y[:train_size], y[train_size:]\n",
        "\n",
        "# Build the LSTM model\n",
        "model = Sequential()\n",
        "model.add(LSTM(units=50, return_sequences=True, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
        "model.add(LSTM(units=50))\n",
        "model.add(Dense(1))\n",
        "\n",
        "model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, epochs=50, batch_size=32)\n",
        "\n",
        "# Predict the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Invert scaling for predictions and true values\n",
        "# To invert the scaling, concatenate the features (X_test) with the predictions and true values\n",
        "y_test_rescaled = scaler.inverse_transform(np.concatenate((X_test[:, -1, :-1], y_test.reshape(-1, 1)), axis=1))[:, -1]\n",
        "y_pred_rescaled = scaler.inverse_transform(np.concatenate((X_test[:, -1, :-1], y_pred), axis=1))[:, -1]\n",
        "\n",
        "# Evaluation\n",
        "mae = np.mean(np.abs(y_test_rescaled - y_pred_rescaled))\n",
        "rmse = np.sqrt(np.mean((y_test_rescaled - y_pred_rescaled)**2))\n",
        "\n",
        "print(f'Mean Absolute Error: {mae}')\n",
        "print(f'Root Mean Squared Error: {rmse}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "daQDMb3Gerpw",
        "outputId": "a64810b3-499f-4be6-8b85-f3a7ec08e6a9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9ms/step - loss: 0.0115\n",
            "Epoch 2/50\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 8ms/step - loss: 0.0068\n",
            "Epoch 3/50\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - loss: 0.0063\n",
            "Epoch 4/50\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - loss: 0.0059\n",
            "Epoch 5/50\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - loss: 0.0053\n",
            "Epoch 6/50\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 0.0049\n",
            "Epoch 7/50\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.0045\n",
            "Epoch 8/50\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - loss: 0.0040\n",
            "Epoch 9/50\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - loss: 0.0040\n",
            "Epoch 10/50\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 0.0038\n",
            "Epoch 11/50\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - loss: 0.0036\n",
            "Epoch 12/50\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - loss: 0.0033\n",
            "Epoch 13/50\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - loss: 0.0034\n",
            "Epoch 14/50\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 8ms/step - loss: 0.0035\n",
            "Epoch 15/50\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - loss: 0.0032\n",
            "Epoch 16/50\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - loss: 0.0033\n",
            "Epoch 17/50\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - loss: 0.0031\n",
            "Epoch 18/50\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.0030\n",
            "Epoch 19/50\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 0.0029\n",
            "Epoch 20/50\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - loss: 0.0029\n",
            "Epoch 21/50\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 0.0028\n",
            "Epoch 22/50\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - loss: 0.0028\n",
            "Epoch 23/50\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - loss: 0.0027\n",
            "Epoch 24/50\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - loss: 0.0026\n",
            "Epoch 25/50\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - loss: 0.0027\n",
            "Epoch 26/50\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - loss: 0.0026\n",
            "Epoch 27/50\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - loss: 0.0025\n",
            "Epoch 28/50\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - loss: 0.0022\n",
            "Epoch 29/50\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 0.0022\n",
            "Epoch 30/50\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - loss: 0.0022\n",
            "Epoch 31/50\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 0.0021\n",
            "Epoch 32/50\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - loss: 0.0021\n",
            "Epoch 33/50\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 0.0019\n",
            "Epoch 34/50\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - loss: 0.0019\n",
            "Epoch 35/50\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 0.0019\n",
            "Epoch 36/50\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - loss: 0.0018\n",
            "Epoch 37/50\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - loss: 0.0018\n",
            "Epoch 38/50\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - loss: 0.0018\n",
            "Epoch 39/50\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - loss: 0.0018\n",
            "Epoch 40/50\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - loss: 0.0016\n",
            "Epoch 41/50\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - loss: 0.0016\n",
            "Epoch 42/50\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 0.0015\n",
            "Epoch 43/50\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 8ms/step - loss: 0.0015\n",
            "Epoch 44/50\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 0.0015\n",
            "Epoch 45/50\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 7ms/step - loss: 0.0014\n",
            "Epoch 46/50\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 0.0014\n",
            "Epoch 47/50\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 8ms/step - loss: 0.0014\n",
            "Epoch 48/50\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - loss: 0.0012\n",
            "Epoch 49/50\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - loss: 0.0013\n",
            "Epoch 50/50\n",
            "\u001b[1m769/769\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - loss: 0.0014\n",
            "\u001b[1m193/193\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step\n",
            "Mean Absolute Error: 12.14944732257655\n",
            "Root Mean Squared Error: 22.935079434166692\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "# Save the model\n",
        "model.save('stock_prediction_model.h5')\n",
        "\n",
        "# Download the saved model\n",
        "files.download('stock_prediction_model.h5')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "AVJYLGPIhX0n",
        "outputId": "5c1d9303-a3f4-4f31-add5-b0d8191e173b"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_31ea25cb-7a9c-461d-bc5d-da4b4778b434\", \"stock_prediction_model.h5\", 417288)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "# Your Finnhub API key\n",
        "api_key = 'cua9athr01qkpes4gsh0cua9athr01qkpes4gshg'\n",
        "\n",
        "# Base URL for Finnhub API\n",
        "base_url = 'https://finnhub.io/api/v1/'\n",
        "\n",
        "# Example endpoint: Getting the current price for a stock\n",
        "symbol = 'TTM'  # Example: Apple stock\n",
        "\n",
        "# Construct the URL for the request\n",
        "url = f\"{base_url}quote?symbol={symbol}&token={api_key}\"\n",
        "\n",
        "# Make the GET request\n",
        "response = requests.get(url)\n",
        "\n",
        "# Check if the request was successful (status code 200)\n",
        "if response.status_code == 200:\n",
        "    data = response.json()  # Parse the JSON response\n",
        "    print(f\"Current Price Data for {symbol}:\")\n",
        "    print(data)\n",
        "else:\n",
        "    print(f\"Error: {response.status_code}, Message: {response.text}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52LNtTQVhYVM",
        "outputId": "7fcc7c8d-4968-4b59-ed06-5d4f0d16455a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Price Data for TTM:\n",
            "{'c': 0, 'd': None, 'dp': None, 'h': 0, 'l': 0, 'o': 0, 'pc': 0, 't': 0}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Your Finnhub API key\n",
        "api_key = 'cua9athr01qkpes4gsh0cua9athr01qkpes4gshg'\n",
        "\n",
        "# Base URL for Finnhub API\n",
        "base_url = 'https://finnhub.io/api/v1/'\n",
        "\n",
        "# Example endpoint: Getting the current price for a stock\n",
        "symbol = 'HIG'  # Example: Hartford stock\n",
        "\n",
        "# Construct the URL for the request\n",
        "url = f\"{base_url}quote?symbol={symbol}&token={api_key}\"\n",
        "\n",
        "# Make the GET request\n",
        "response = requests.get(url)\n",
        "\n",
        "# Check if the request was successful (status code 200)\n",
        "if response.status_code == 200:\n",
        "    data = response.json()  # Parse the JSON response\n",
        "    print(f\"Current Price Data for {symbol}:\")\n",
        "    print(data)\n",
        "else:\n",
        "    print(f\"Error: {response.status_code}, Message: {response.text}\")\n",
        "\n",
        "# Extract relevant data from the Finnhub API response\n",
        "current_price = data['c']  # Current price (c)\n",
        "open_price = data['o']  # Open price (o)\n",
        "high_price = data['h']  # High price (h)\n",
        "low_price = data['l']  # Low price (l)\n",
        "prev_close = data['pc']  # Previous close (pc)\n",
        "\n",
        "# For the moving averages, we need the most recent historical close prices\n",
        "# Assuming you have this data already or can retrieve it. Here we'll use some placeholders.\n",
        "# You would need the most recent 'Close' prices from your data.\n",
        "last_close_prices = np.array([110.90, 111.00, 110.80, 110.70, 110.75, 110.85, 111.10, 111.25, 111.30, 111.50])  # Example close prices\n",
        "\n",
        "# Calculate moving averages\n",
        "MA_5 = np.mean(last_close_prices[-5:])  # Simple moving average for the last 5 closes\n",
        "MA_20 = np.mean(last_close_prices[-20:]) if len(last_close_prices) >= 20 else np.mean(last_close_prices)  # MA_20 (fallback to MA_5 if not enough data)\n",
        "\n",
        "# Create a DataFrame for the current data\n",
        "current_data = pd.DataFrame({\n",
        "    'Open': [open_price],\n",
        "    'High': [high_price],\n",
        "    'Low': [low_price],\n",
        "    'Volume': [1000000],  # Placeholder for Volume (replace with actual if available)\n",
        "    'MA_5': [MA_5],\n",
        "    'MA_20': [MA_20]\n",
        "})\n",
        "\n",
        "# Normalize the data using the same scaler that was used during training\n",
        "# Ensure you have the trained scaler (for example, save it and reload when needed)\n",
        "scaler = MinMaxScaler(feature_range=(0, 1))\n",
        "\n",
        "# Scale the current data\n",
        "scaled_data = scaler.fit_transform(current_data)\n",
        "\n",
        "# Prepare the input for the LSTM model\n",
        "look_back = 60  # The model was trained with a look-back of 60 time steps\n",
        "\n",
        "# Since we only have one data point, we need to repeat it to create a sequence of `look_back` length\n",
        "X_input = np.repeat(scaled_data, look_back, axis=0).reshape(1, look_back, len(current_data.columns))\n",
        "\n",
        "# Predict using the trained LSTM model\n",
        "y_pred = model.predict(X_input)\n",
        "\n",
        "# Invert scaling for the prediction (predicted close price)\n",
        "# Reshape y_pred to be 1D, and concatenate it with the last features of X_input\n",
        "y_pred_rescaled = scaler.inverse_transform(np.concatenate(\n",
        "    (X_input[0, -1, :-1], y_pred.reshape(-1)), axis=0).reshape(1, -1))[:, -1]\n",
        "predicted_close = y_pred_rescaled[0]\n",
        "\n",
        "# Show the predicted close price\n",
        "print(f\"Predicted Close Price: {predicted_close}\")\n",
        "\n",
        "\n",
        "# Step 3: Decision on investment\n",
        "if predicted_close > current_price:\n",
        "    print(\"It might be a good time to invest, as the predicted price is higher than the current price.\")\n",
        "else:\n",
        "    print(\"It might not be a good time to invest, as the predicted price is lower than or close to the current price.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ekA0Mikkp8S",
        "outputId": "699835b1-a5e9-44ec-8cc0-1189ab812444"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current Price Data for HIG:\n",
            "{'c': 111.49, 'd': 0.6, 'dp': 0.5411, 'h': 111.66, 'l': 110.55, 'o': 111.02, 'pc': 110.89, 't': 1737752400}\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "Predicted Close Price: 111.05086436808108\n",
            "It might not be a good time to invest, as the predicted price is lower than or close to the current price.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lYaLRE-vk8Dt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}